{
  "url": "https://github.com/salesforce/WikiSQL",
  "text": "A large crowd-sourced dataset for developing natural language interfaces for relational databases. WikiSQL is the dataset released along with our work Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning.\nIf you use WikiSQL, please cite the following work:\nVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning.\ntitle = {Seq2SQL: Generating Structured Queries from Natural Language using\nRegarding tokenization and Stanza --- when WikiSQL was written 3-years ago, it relied on Stanza, a CoreNLP python wrapper that has since been deprecated. If you'd still like to use the tokenizer, please use the docker image. We do not anticipate switching to the current Stanza as changes to the tokenizer would render the previous results not reproducible.\nIf you submit papers on WikiSQL, please consider sending a pull request to merge your results onto the leaderboard. By submitting, you acknowledge that your results are obtained purely by training on the training split and tuned on the dev split (e.g. you only evaluted on the test set once). Moreover, you acknowledge that your models only use the table schema and question during inference. That is they do not use the table content. Update (May 12, 2019): We now have a separate leaderboard for weakly supervised models that do not use logical forms during training.\n| Model | Dev execution accuracy | Test execution accuracy |\n| LatentAlignment (Wang 2019) | 79.4 | 79.3 |\n| MeRL (Agarwal 2019) | 74.9 +/- 0.1 | 74.8 +/- 0.2 |\n| MAPO (Liang 2018) | 72.2 +/- 0.2 | 72.1 +/- 0.3 |\n| Rule-SQL (Guo 2019) | 61.1 +/- 0.2 | 61.0 +/- 0.3 |\n| SeaD +Execution-Guided Decoding (Xu 2021) (Ant Group, Ada & ZhiXiaoBao) |\n| SDSQL +Execution-Guided Decoding (Hui 2020) (Alibaba Group) |\n| IE-SQL +Execution-Guided Decoding (Ma 2020) (Ping An Life, AI Team) |\n| HydraNet +Execution-Guided Decoding (Lyu 2020) (Microsoft Dynamics 365 AI) (code) |\n| BRIDGE^ +Execution-Guided Decoding (Lin 2020) (Salesforce Research) |\n| X-SQL +Execution-Guided Decoding (He 2019) |\n| BRIDGE^ (Lin 2020) (Salesforce Research) |\n| Text2SQLGen + EG (Mellah 2021) (Novelis.io Research) |\n| SeqGenSQL+EG (Li 2020) | 90.8 | 90.5 | Inference | ||\n| SeqGenSQL (Li 2020) | 90.6 | 90.3 | Inference | ||\n| SeaD (Xu 2021) (Ant Group, Ada & ZhiXiaoBao) |\n| (Guo 2019) +Execution-Guided Decoding with BERT-Base-Uncased^ |\n| SQLova +Execution-Guided Decoding (Hwang 2019) |\n| IncSQL +Execution-Guided Decoding (Shi 2018) |\n| HydraNet (Lyu 2020) (Microsoft Dynamics 365 AI) (code) |\n| IE-SQL (Ma 2020) (Ping An Life, AI Team) |\n| Execution-Guided Decoding (Wang 2018) |\n| (Guo 2018) | 64.1 | 71.1 | 62.5 | 69.0 | |\n| Wang 2017^ | 62.0 | 67.1 | 61.5 | 66.8 | |\nindicates that table content is used directly by the model during training.\nindicates that the order in where conditions is ignored.\nBoth the evaluation script as well as the dataset are stored within the repo. Only Python 3 is supported at the moment - I would very much welcome a pull request that ports the code to work with Python 2. The installation steps are as follows:\ngit clone https://github.com/salesforce/WikiSQL\nThis will unpack the data files into a directory called data\nInside the data folder you will find the files in jsonl\nThe former can be read line by line, where each line is a serialized JSON object.\n\"question\":\"who is the manufacturer for the order year 1998?\",\n: the phase in which the dataset was collected. We collected WikiSQL in two phases.question\n: the natural language question written by the worker.table_id\n: the ID of the table to which this question is addressed.sql\n: the SQL query corresponding to the question. This has the following subfields:sel\n: the numerical index of the column that is being selected. You can find the actual column from the table.agg\n: the numerical index of the aggregation operator that is being used. You can find the actual operator fromQuery.agg_ops\n: a list of triplets(column_index, operator_index, condition)\n: the numerical index of the condition column that is being used. You can find the actual column from the table.operator_index\n: the numerical index of the condition operator that is being used. You can find the actual operator fromQuery.cond_ops\n: the comparison value for the condition, in eitherstring\nThese files are contained in the *.tables.jsonl\n\"ACT \\u00b7 CELEBRATION OF A CENTURY 2013\",\n\"Current series will be exhausted this year\"\n: a list of column names in the table.rows\n: a list of rows. Each row is a list of row entries.\nTables are also contained in a corresponding *.db\nThis is a SQL database with the same information.\nNote that due to the flexible format of HTML tables, the column names of tables in the database has been symbolized.\nFor example, for a table with the columns ['foo', 'bar']\n, the columns in the database are actually col0\ncontains the evaluation script, whose options are:\nusage: evaluate.py [-h] source_file db_file pred_file\nsource_file source file for the prediction\ndb_file source database for the prediction\n-h, --help show this help message and exit\n, which is supplied by the user, should contain lines of serialized JSON objects.\nfield which corresponds to the query predicted for a line in the input *.jsonl\n: the numerical index of the column that is being selected. You can find the actual column from the table.agg\n: the numerical index of the aggregation operator that is being used. You can find the actual operator fromQuery.agg_ops\n: a list of triplets(column_index, operator_index, condition)\n: the numerical index of the condition column that is being used. You can find the actual column from the table.operator_index\n: the numerical index of the condition operator that is being used. You can find the actual operator fromQuery.cond_ops\n: the comparison value for the condition, in eitherstring\nAn example predictions file can be found in test/example.pred.dev.jsonl\ndirectory contains dependencies of evaluate.py\nWe supply a sample predictions file for the dev set in test/example.pred.dev.jsonl.bz2\nYou can unzip this file using bunzip2 test/example.pred.dev.jsonl.bz2 -k\nto look at what a real predictions file should look like.\nWe distribute a docker file which installs the necessary dependencies of this library and runs the evaluation script on this file.\nThe docker file also serves as an example of how to use the evaluation script.\nTo run the test, first build the image from the root directory:\ndocker build -t wikisqltest -f test/Dockerfile .\ndocker run --rm --name wikisqltest wikisqltest\nIf everything works correctly, the output should be:\nIn addition to the raw data dump, we also release an optional annotation script that annotates WikiSQL using Stanford CoreNLP.\nscript will annotate the query, question, and SQL table, as well as a sequence to sequence construction of the input and output for convenience of using Seq2Seq models.\n, you must set up the CoreNLP python client using Stanford Stanza.\nOne docker image of the CoreNLP server that this works with is here:\ndocker run --name corenlp -d -p 9000:9000 vzhong/corenlp-server\nNote that the sequence output contain symbols to delineate the boundaries of fields.\nyou will also find accompanying functions to reconstruct a query given a sequence output in the annotated format.\nI will update this list with frequently asked questions.\nHow do you convert HTML table columns to SQL table columns?\nWeb tables are noisy and are not directly transferrable into a database. One problem is that SQL column names need to be symbolic whereas web table columns usually have unicode characters, whitespaces etc. To handle this problem, we convert table columns to symbols (e.g.\n) just before executing the query. For the implementation details, please seeevaluate.py\n- 1.1: Removed examples from each split that have gloss mismatch between the logical form conditions and the annotated question utterance."
}